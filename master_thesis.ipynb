{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup & Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shap\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import entropy\n",
    "from sklearn.linear_model import LinearRegression\n",
    "pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
    "from category_encoders import TargetEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "from scipy.stats.mstats import winsorize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.impute import SimpleImputer\n",
    "# import accuracy_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# import sensitivity_score\n",
    "from sklearn.metrics import recall_score\n",
    "# import precision_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from tqdm import tqdm\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'crunchbase/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "orgs = pd.read_csv(directory + 'organizations.csv', usecols=['uuid', 'name', 'country_code', 'state_code', 'roles', 'primary_role',\n",
    "                                                                   'status', 'category_list', 'category_groups_list', \n",
    "                                                                   'founded_on', 'closed_on'])\n",
    "\n",
    "acquisitions = pd.read_csv(directory + 'acquisitions.csv', usecols=['acquiree_uuid', 'acquired_on'])\n",
    "\n",
    "rounds = pd.read_csv(directory + 'funding_rounds.csv', usecols=['org_uuid', 'state_code', 'investment_type', 'announced_on', 'raised_amount_usd',\n",
    "                                                                'investor_count'])\n",
    "\n",
    "ipos = pd.read_csv(directory + 'ipos.csv', usecols=['org_uuid', 'went_public_on'])\n",
    "\n",
    "people = pd.read_csv(directory + 'people.csv', usecols=['uuid', 'name', 'gender', 'country_code', 'featured_job_title', 'featured_job_organization_name', 'featured_job_organization_uuid'])\n",
    "\n",
    "jobs = pd.read_csv(directory + 'jobs.csv', usecols=['person_uuid', 'org_uuid', 'title', 'started_on', 'ended_on'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_basis(rounds):\n",
    "    basis = rounds[rounds['investment_type'].isin(['series_a', 'series_b', 'pre_seed', 'seed', 'angel'])] \n",
    "    basis = basis.drop_duplicates(subset=['org_uuid', 'investment_type'], keep='first')\n",
    "    pivot_df = basis.pivot_table(\n",
    "        index='org_uuid',\n",
    "        columns='investment_type',\n",
    "        values=['announced_on', 'raised_amount_usd', 'investor_count', 'state_code'],\n",
    "        aggfunc={\n",
    "            'announced_on': 'first', \n",
    "            'raised_amount_usd': 'sum', \n",
    "            'investor_count': 'sum', \n",
    "            'state_code': 'first',\n",
    "        }\n",
    "    )\n",
    "\n",
    "    pivot_df.columns = [' '.join(col).strip() for col in pivot_df.columns.values]\n",
    "    pivot_df = pivot_df.drop(columns='raised_amount_usd series_b')\n",
    "    pivot_df = pivot_df.drop(columns='investor_count series_b')\n",
    "    pivot_df.columns = pivot_df.columns.str.replace(' ', '_')\n",
    "    pivot_df = pivot_df.reset_index()\n",
    "    pivot_df[[ 'investor_count_angel', 'investor_count_pre_seed',\n",
    "       'investor_count_seed', 'investor_count_series_a',\n",
    "       'raised_amount_usd_angel', 'raised_amount_usd_pre_seed',\n",
    "       'raised_amount_usd_seed', 'raised_amount_usd_series_a']] = pivot_df[[ 'investor_count_angel', 'investor_count_pre_seed',\n",
    "       'investor_count_seed', 'investor_count_series_a',\n",
    "       'raised_amount_usd_angel', 'raised_amount_usd_pre_seed',\n",
    "       'raised_amount_usd_seed', 'raised_amount_usd_series_a']].fillna(0)\n",
    "    \n",
    "    print(len(pivot_df))\n",
    "    pivot_df = pivot_df[pivot_df['raised_amount_usd_series_a'] != 0]\n",
    "    print(len(pivot_df))\n",
    "    pivot_df = pivot_df.sort_values(by='org_uuid', ascending=True)\n",
    "\n",
    "    pivot_df['temp_series_a_month'] = pd.to_datetime(pivot_df['announced_on_series_a'])\n",
    "\n",
    "    pivot_df['temp_series_a_month'] = pivot_df['temp_series_a_month'].dt.to_period('M')\n",
    "    print(len(pivot_df))\n",
    "\n",
    "    pivot_df = pivot_df[pivot_df['temp_series_a_month'] < '2019-05']\n",
    "    print(len(pivot_df))\n",
    "\n",
    "    pivot_df.drop(columns=['temp_series_a_month'], inplace=True)\n",
    "\n",
    "\n",
    "    return pivot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergings_of_basics(orgs, ipos, basis, acquisitions):\n",
    "    basis.set_index('org_uuid', inplace=True)\n",
    "\n",
    "    orgs = orgs.drop_duplicates(subset=['uuid'], keep='first')\n",
    "    orgs = orgs.rename(columns={'uuid': 'org_uuid'})\n",
    "    orgs.sort_values(by='org_uuid', ascending=True)\n",
    "    orgs.set_index('org_uuid', inplace=True)\n",
    "\n",
    "    ipos = ipos.drop_duplicates(subset=['org_uuid'], keep='first')\n",
    "    ipos.sort_values(by='org_uuid', ascending=True)\n",
    "    ipos.set_index('org_uuid', inplace=True)\n",
    "\n",
    "    acquisitions = acquisitions.drop_duplicates(subset=['acquiree_uuid'], keep='first')\n",
    "    acquisitions = acquisitions.rename(columns={'acquiree_uuid': 'org_uuid'})\n",
    "    acquisitions.sort_values(by='org_uuid', ascending=True)\n",
    "    acquisitions.set_index('org_uuid', inplace=True)\n",
    "\n",
    "    basis_plus_orgs = basis.join(orgs, on='org_uuid', how='inner')\n",
    "    basis_plus_orgs_plus_ipos = basis_plus_orgs.join(ipos, on='org_uuid', how='left')\n",
    "    final = basis_plus_orgs_plus_ipos.join(acquisitions, on='org_uuid', how='left')\n",
    "\n",
    "    final = final.reset_index()\n",
    "\n",
    "    to_datetime = ['announced_on_angel', 'announced_on_pre_seed', 'announced_on_seed', 'announced_on_series_a', 'announced_on_series_b', 'founded_on', 'closed_on', 'went_public_on', 'acquired_on']\n",
    "\n",
    "    for col in to_datetime:\n",
    "        final[col] = pd.to_datetime(final[col], errors='coerce')    \n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_data(orgs, ipos, rounds, acquisitions):\n",
    "    basis = create_basis(rounds)\n",
    "    joined_data = mergings_of_basics(orgs, ipos, basis, acquisitions)\n",
    "\n",
    "    # Create binary variable indicating whether the company has been acquired within 5 years after series A\n",
    "    joined_data['acquired_within_5_years'] = (joined_data['acquired_on'] - joined_data['announced_on_series_a']) < pd.Timedelta(days=1825)\n",
    "    joined_data['acquired_within_5_years'] = joined_data['acquired_within_5_years'].astype(int)\n",
    "\n",
    "    # Create binary variable indicating whether the company has gone public within 5 years after series A\n",
    "    joined_data['went_public_within_5_years'] = (joined_data['went_public_on'] - joined_data['announced_on_series_a']) < pd.Timedelta(days=1825)\n",
    "    joined_data['went_public_within_5_years'] = joined_data['went_public_within_5_years'].astype(int)\n",
    "\n",
    "    # Create binary variable indicating whether the company has reached series B within 5 years after series A\n",
    "    joined_data['reached_series_b_within_5_years'] = (joined_data['announced_on_series_b'] - joined_data['announced_on_series_a']) < pd.Timedelta(days=1825)\n",
    "    joined_data['reached_series_b_within_5_years'] = joined_data['reached_series_b_within_5_years'].astype(int)\n",
    "\n",
    "    # Create binary variable indicating whether the company has been closed within 5 years after series A\n",
    "    joined_data['closed_within_5_years'] = (joined_data['closed_on'] - joined_data['announced_on_series_a']) < pd.Timedelta(days=1825)\n",
    "    joined_data['closed_within_5_years'] = joined_data['closed_within_5_years'].astype(int)\n",
    "\n",
    "    joined_data.rename(columns={'reached_series_b_within_5_years': 'successful'}, inplace=True)\n",
    "\n",
    "    joined_data.loc[joined_data['closed_within_5_years'] == 1, 'successful'] = 0\n",
    "    joined_data.loc[joined_data['acquired_within_5_years'] == 1, 'successful'] = 1\n",
    "    joined_data.loc[joined_data['went_public_within_5_years'] == 1, 'successful'] = 1\n",
    "\n",
    "    # Drop columns that are no longer needed\n",
    "    joined_data.drop(columns=['closed_on', 'closed_within_5_years', 'acquired_on', 'acquired_within_5_years', 'went_public_on', 'went_public_within_5_years'], inplace=True)\n",
    "\n",
    "    joined_data.set_index('org_uuid', inplace=True)\n",
    "\n",
    "    return joined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shape_data(orgs, ipos, rounds, acquisitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_founding_dates = pd.read_csv(directory + 'funding_rounds.csv', usecols=['org_uuid', 'state_code', 'investment_type', 'announced_on', 'raised_amount_usd',\n",
    "                                                                'investor_count'])\n",
    "\n",
    "mapping_founding_dates = create_basis(mapping_founding_dates)\n",
    "\n",
    "orgs_date = pd.read_csv(directory + 'organizations.csv', usecols=['uuid', 'founded_on'])\n",
    "orgs_date.rename(columns={'uuid': 'org_uuid'}, inplace=True)\n",
    "\n",
    "mapping_founding_dates = pd.merge(mapping_founding_dates, orgs_date, on='org_uuid', how='inner')\n",
    "\n",
    "mapping_founding_dates = mapping_founding_dates[['org_uuid', 'announced_on_series_a', 'founded_on']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_jobs_people(df, people2, jobs2, mapping_founding_dates):\n",
    "    jobs3 = pd.merge(jobs2, mapping_founding_dates, on='org_uuid', how='left')\n",
    "\n",
    "    jobs3['is_founder'] = jobs3['title'].str.contains('founder', case=False, na=False).astype(int)\n",
    "\n",
    "    jobs3.drop(columns=['title'], inplace=True)\n",
    "\n",
    "    jobs3 = jobs3.loc[(jobs3['started_on'] <= jobs3['announced_on_series_a']) | jobs3['started_on'].isnull()]\n",
    "\n",
    "    jobs3 = jobs3.loc[~(jobs3['ended_on'] <= jobs3['announced_on_series_a'])]\n",
    "\n",
    "    # if started_on is NaN and person is not a founder, then drop observation\n",
    "    jobs3 = jobs3.loc[~(jobs3['started_on'].isnull() & (jobs3['is_founder'] == 0))]\n",
    "\n",
    "    jobs3.drop(columns=['started_on', 'ended_on'], inplace=True)\n",
    "\n",
    "    jobs3.rename(columns={'person_uuid': 'uuid'}, inplace=True)\n",
    "\n",
    "    # group by org_uuid and person uuid, if one of the \"is_founder\" is 1, then the person is a founder\n",
    "    jobs3 = jobs3.groupby(['org_uuid', 'uuid']).agg({'is_founder': 'max'}).reset_index()\n",
    "\n",
    "    # drop duplicates wrt uuid and org_uuid\n",
    "    jobs3 = jobs3.drop_duplicates(subset=['org_uuid', 'uuid'], keep='first')\n",
    "\n",
    "    # add column number of employees, accordingly group by org_uuid and count the number of employees. \n",
    "    jobs3['number_of_employees'] = jobs3.groupby('org_uuid')['uuid'].transform('count')\n",
    "\n",
    "    # delete non-founder observations\n",
    "    jobs3 = jobs3.loc[jobs3['is_founder'] == 1]\n",
    "\n",
    "    df_merge = df.copy()\n",
    "\n",
    "    df_merge = df_merge.reset_index()\n",
    "\n",
    "    df_merge = pd.merge(jobs3, df_merge, on='org_uuid', how='left')\n",
    "\n",
    "    # only keep columns org_uuid, uuid, successful, series a date, and number of employees\n",
    "    df_merge = df_merge[['org_uuid', 'uuid', 'successful', 'announced_on_series_a', 'number_of_employees']]\n",
    "\n",
    "    df_merge['number_of_companies_founded_before'] = df_merge.groupby('uuid')['announced_on_series_a'].rank(method='first') - 1\n",
    "\n",
    "    df_merge_test = df_merge.copy()\n",
    "    df_merge_test['number_of_successful_companies_founded_before'] = 0\n",
    "\n",
    "    for founder, rows in df_merge.groupby('uuid'):\n",
    "        # sort the group by the series a date\n",
    "        rows = rows.sort_values(by='announced_on_series_a', ascending=True)\n",
    "        for index, row in rows.iterrows():\n",
    "            if row['number_of_companies_founded_before'] == 0:\n",
    "                continue\n",
    "            else:\n",
    "                # get the number of successful companies founded before\n",
    "                df_merge_test.loc[index, 'number_of_successful_companies_founded_before'] = rows.loc[rows['announced_on_series_a'] < row['announced_on_series_a'] - pd.DateOffset(years=5), 'successful'].sum()\n",
    "\n",
    "    people2.drop(columns=['name', 'featured_job_title', 'featured_job_organization_name', 'featured_job_organization_uuid'], inplace=True)\n",
    "\n",
    "    people_and_jobs = pd.merge(df_merge_test, people2, on=['uuid'], how='left')\n",
    "\n",
    "    people_and_jobs['male_founders'] = (people_and_jobs['gender'] == 'male').astype(int)\n",
    "    people_and_jobs['female_founders'] = (people_and_jobs['gender'] == 'female').astype(int)\n",
    "\n",
    "    pj_test = pd.merge(people_and_jobs, mapping_founding_dates, on='org_uuid', how='left')\n",
    "\n",
    "    pj_test.rename(columns={'uuid': 'founder_uuid', 'country_code': 'founders_country_code'}, inplace=True)\n",
    "\n",
    "    pj_test.drop(columns=['announced_on_series_a_x', 'successful'], inplace=True)\n",
    "\n",
    "    pj_test.set_index('founder_uuid', inplace=True)\n",
    "\n",
    "    successes = pj_test.copy()\n",
    "\n",
    "    successes.drop(columns=['gender', 'founders_country_code', 'male_founders', 'female_founders', 'announced_on_series_a_y', 'founded_on'], inplace=True)\n",
    "\n",
    "    successes.columns\n",
    "\n",
    "    successes['success_ratio'] = successes['number_of_successful_companies_founded_before'] / successes['number_of_companies_founded_before']\n",
    "\n",
    "    successes.reset_index(inplace=True)\n",
    "\n",
    "    successes = successes.drop(columns=['founder_uuid'])\n",
    "\n",
    "    # lets aggregate this on a company level\n",
    "    tesa = successes.groupby('org_uuid').agg({\n",
    "    'number_of_employees': 'max',\n",
    "    'number_of_companies_founded_before': ['mean', 'max', 'min'], \n",
    "    'number_of_successful_companies_founded_before': ['mean', 'max'],\n",
    "    'success_ratio': ['mean', 'max']\n",
    "    })\n",
    "\n",
    "    # fill NA over the whole df with 0\n",
    "    tesa = tesa.fillna(0)\n",
    "\n",
    "    # lets get rid of the multiindex\n",
    "    tesa.columns = ['_'.join(col).strip() for col in tesa.columns.values]\n",
    "\n",
    "    tesa = tesa.reset_index()\n",
    "\n",
    "    # now lets get tesa into df\n",
    "    df = df.reset_index()\n",
    "    df = pd.merge(df, tesa, on='org_uuid', how='left')\n",
    "\n",
    "    \"\"\"pj_test.drop(columns=['number_of_companies_founded_before', 'number_of_successful_companies_founded_before', \n",
    "                          'success_ratio', 'number_of_employees'], inplace=True)\"\"\"\n",
    "\n",
    "    return df, pj_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, pj_test = preprocess_jobs_people(df, people, jobs, mapping_founding_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_test_copy = pj_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_copy[['number_of_employees_max', 'number_of_companies_founded_before_mean', 'number_of_companies_founded_before_max', 'number_of_companies_founded_before_min', 'number_of_successful_companies_founded_before_mean', 'number_of_successful_companies_founded_before_max', 'success_ratio_mean', 'success_ratio_max']] = df_copy[['number_of_employees_max', 'number_of_companies_founded_before_mean', 'number_of_companies_founded_before_max', 'number_of_companies_founded_before_min', 'number_of_successful_companies_founded_before_mean', 'number_of_successful_companies_founded_before_max', 'success_ratio_mean', 'success_ratio_max']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pj_test_copy.drop(columns=['number_of_companies_founded_before', 'number_of_successful_companies_founded_before', 'number_of_employees'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = pd.read_csv(directory + 'degrees.csv', usecols=['person_uuid', 'subject', \n",
    "                                                          'started_on', 'completed_on'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(degrees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_degrees(degrees, founders):\n",
    "    degrees.rename(columns={'person_uuid': 'founder_uuid'}, inplace=True)\n",
    "    degrees.set_index('founder_uuid', inplace=True)\n",
    "    grads = founders.join(degrees, on='founder_uuid', how='left')\n",
    "\n",
    "    grads['started_on'] = pd.to_datetime(grads['started_on'], errors='coerce')\n",
    "    grads['completed_on'] = pd.to_datetime(grads['completed_on'], errors='coerce')\n",
    "    grads['announced_on_series_a'] = pd.to_datetime(grads['announced_on_series_a_y'], errors='coerce')\n",
    "    grads['founded_on'] = pd.to_datetime(grads['founded_on'], errors='coerce')\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_on_founder_level(grads, founders):\n",
    "\n",
    "    grads = preprocessing_degrees(degrees, founders)\n",
    "    grads_all = grads.copy()\n",
    "\n",
    "    grads = grads[grads['started_on'] < grads['announced_on_series_a']]\n",
    "    \n",
    "    # if grads['subject] includes \"computer science\" or \"engineering\" or \"analytics\", then new column \"coding_experience\" = 1\n",
    "    grads['coding_experience'] = grads['subject'].str.contains('computer|engineering|analytics|information|software|robotics|artificial intelligence',\n",
    "                                                            case=False, na=False).astype(int)\n",
    "\n",
    "    # if it includes business, economics, finance, marketing, then new column \"business_experience\" = 1\n",
    "    grads['business_experience'] = grads['subject'].str.contains('business|economics|finance|marketing|management|entrepreneurship|accounting', \n",
    "                                                             case=False, na=False).astype(int)\n",
    "    \n",
    "    # if the completion date is after the series a, then the degree is not completed. accordingly, set the completion date to NA\n",
    "    grads.loc[grads['completed_on'] > grads['announced_on_series_a'], 'completed_on'] = pd.NaT\n",
    "\n",
    "    grads['number_of_degrees_started'] = grads.groupby(['founder_uuid', 'org_uuid'])['started_on'].transform('count')\n",
    "\n",
    "    grads['coding_experience_f'] = grads.groupby(['founder_uuid', 'org_uuid'])['coding_experience'].transform('max')\n",
    "    grads['business_experience_f'] = grads.groupby(['founder_uuid', 'org_uuid'])['business_experience'].transform('max')\n",
    "\n",
    "    grads['this_degree_completed'] = grads['completed_on'] < grads['announced_on_series_a']\n",
    "    grads['this_degree_completed'] = grads['this_degree_completed'].astype(int)\n",
    "\n",
    "    grads['number_of_degrees_completed'] = grads.groupby(['founder_uuid', 'org_uuid'])['this_degree_completed'].transform('sum')\n",
    "\n",
    "    grads['time_spent_studying'] = grads.groupby(['founder_uuid', 'org_uuid'])['completed_on'].transform('max') - grads.groupby(['founder_uuid', 'org_uuid'])['started_on'].transform('min')\n",
    "\n",
    "    grads['time_spent_studying'] = grads['time_spent_studying'].dt.days\n",
    "\n",
    "    grads['time_since_graduation_to_founding'] = grads['founded_on'] - grads.groupby(['founder_uuid', 'org_uuid'])['completed_on'].transform('max')\n",
    "\n",
    "    grads['time_since_graduation_to_founding'] = grads['time_since_graduation_to_founding'].dt.days\n",
    "\n",
    "    grads['time_spent_studying_per_degree'] = grads['time_spent_studying']/grads['number_of_degrees_started']\n",
    "\n",
    "    grads.reset_index(inplace=True)\n",
    "\n",
    "    grads_all = grads_all.merge(grads[['founder_uuid', 'org_uuid', 'coding_experience_f', 'business_experience_f', 'number_of_degrees_started', 'number_of_degrees_completed', 'time_spent_studying', \n",
    "                                       'time_since_graduation_to_founding', 'time_spent_studying_per_degree']], on=['founder_uuid', 'org_uuid'], how='left')\n",
    "\n",
    "    return grads_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees_copy = degrees.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads = aggregate_on_founder_level(degrees_copy, pj_test_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_entropy(x):\n",
    "    x = x.dropna()\n",
    "    if x.empty:\n",
    "        return 0 \n",
    "    proportions = x.value_counts(normalize=True)\n",
    "    entropy = np.sum([p * np.log2(1/p) if p > 0 else 0 for p in proportions])\n",
    "    return entropy\n",
    "\n",
    "def add_entropy_before_aggregation(grads):\n",
    "    entropy_values = grads.groupby('org_uuid')['founders_country_code'].apply(calculate_entropy).rename('country_code_entropy')\n",
    "    \n",
    "    grads = grads.merge(entropy_values, on='org_uuid', how='left')\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_on_company_level(grads):\n",
    "    grads['number_of_founders'] = grads.groupby('org_uuid')['founder_uuid'].transform('count')\n",
    "    grads = add_entropy_before_aggregation(grads)\n",
    "    aggregated = grads.groupby('org_uuid').agg({\n",
    "        'coding_experience_f': 'sum',\n",
    "        'business_experience_f': 'sum',\n",
    "        'number_of_degrees_started': 'sum',\n",
    "        'number_of_degrees_completed': 'sum',\n",
    "        'time_spent_studying': ['sum', 'mean'],\n",
    "        'time_spent_studying_per_degree': 'mean',\n",
    "        'time_since_graduation_to_founding': ['mean', 'max', 'min'],\n",
    "        'male_founders': 'sum',\n",
    "        'female_founders': 'sum',\n",
    "        'founders_country_code': pd.Series.nunique,\n",
    "        'founder_uuid': 'count',\n",
    "        'country_code_entropy': 'first'\n",
    "    })\n",
    "  \n",
    "\n",
    "    aggregated.columns = ['_'.join(col).strip() for col in aggregated.columns.values]\n",
    "\n",
    "    # solve multicollinearity by introducing new cols: male_founders_proportion and unique_founders_country_code\n",
    "    aggregated['male_founders_proportion'] = aggregated['male_founders_sum']/aggregated['founder_uuid_count']\n",
    "\n",
    "    # degrees per founder\n",
    "    aggregated['degrees_per_founder_started'] = aggregated['number_of_degrees_started_sum']/aggregated['founder_uuid_count']\n",
    "    aggregated['degrees_per_founder_completed'] = aggregated['number_of_degrees_completed_sum']/aggregated['founder_uuid_count']\n",
    "\n",
    "    return aggregated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "comps = aggregate_on_company_level(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df_copy.join(comps, on='org_uuid', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorizer = {'Media and Entertainment': ['MediaandEntertainment', 'Video', 'MusicandAudio', 'Gaming', 'ContentandPublishing'],\n",
    "               \n",
    "               'Technology': ['Platforms', 'Mobile', 'Apps', 'MessagingandTelecommunications',\n",
    "                              'Hardware', 'PrivacyandSecurity', 'ConsumerElectronics', 'InternetServices', 'Payments'],\n",
    "\n",
    "                'Commerce and Business': ['ProfessionalServices', 'AdministrativeServices', 'CommerceandShopping',\n",
    "                                          'SalesandMarketing', 'Manufacturing', 'ConsumerGoods', 'FinancialServices',\n",
    "                                          'LendingandInvestments', 'RealEstate'],\n",
    "\n",
    "                'Social and Community Services': ['SocialImpact', 'CommunityandLifestyle', 'Education'],\n",
    "\n",
    "                'Health and Life Sciences': ['HealthCare', 'AgricultureandFarming'],\n",
    "                \n",
    "                'Environmental and Sustainability': ['Sustainability', 'NaturalResources', 'Energy', 'ScienceandEngineering'],\n",
    "                \n",
    "                'Design and Creativity': ['Design', 'Advertising', 'ClothingandApparel'],\n",
    "                \n",
    "                'Transportation and Travel': ['Transportation', 'TravelandTourism', 'NavigationandMapping'],\n",
    "                \n",
    "                'Food and Beverage': ['FoodandBeverage'],\n",
    "                \n",
    "                'Miscellaneous': ['Other', 'nan', 'Events'],\n",
    "                \n",
    "                'Sports': ['Sports'],\n",
    "                \n",
    "                'Software': ['Software', 'InformationTechnology', 'ArtificialIntelligence(AI)', 'BlockchainandCryptocurrency', 'DataandAnalytics'],\n",
    "                \n",
    "                'GovAndMilitary': ['GovernmentandMilitary'],\n",
    "                \n",
    "                'Biotechnology': ['Biotechnology']\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize(test):\n",
    "    category_groups_list = []\n",
    "\n",
    "    test['category_groups_list'] = test['category_groups_list'].astype(str)\n",
    "    test['category_groups_list'] = test['category_groups_list'].str.replace(' ', '')\n",
    "\n",
    "    for i in test['category_groups_list']:\n",
    "        if type(i) == str:\n",
    "            category_groups_list.extend(i.split(','))\n",
    "\n",
    "    for key in categorizer.keys():\n",
    "        test[key] = 0\n",
    "\n",
    "    for key, value in categorizer.items():\n",
    "        for val in value:\n",
    "            test[key] = test[key] + test['category_groups_list'].str.contains(val).astype(int)\n",
    "\n",
    "    test.drop(columns='category_groups_list', inplace=True)\n",
    "    test.drop(columns='category_list', inplace=True)\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dates(df):\n",
    "    df['founded_year'] = df['founded_on'].dt.year\n",
    "    df['founded_month'] = df['founded_on'].dt.month\n",
    "    df['foundation_dayofyear'] = df['founded_on'].dt.dayofyear\n",
    "    df['founded_quarter'] = df['founded_on'].dt.quarter\n",
    "\n",
    "    df['series_a_year'] = df['series_a_date'].dt.year\n",
    "    df['series_a_month'] = df['series_a_date'].dt.month\n",
    "    df['series_a_dayofyear'] = df['series_a_date'].dt.dayofyear\n",
    "    df['series_a_quarter'] = df['series_a_date'].dt.quarter\n",
    "\n",
    "    df['age_series_a'] = (df['series_a_date'] - df['founded_on']).dt.days\n",
    "\n",
    "    # average time between funding rounds\n",
    "    df['number_of_funding_rounds'] = df['angel'] + df['pre_seed'] + df['seed'] \n",
    "    df['time_between_rounds'] = (df['series_a_date'] - df['founded_on']).dt.days / df['number_of_funding_rounds']\n",
    "\n",
    "\n",
    "    df.drop(columns=['founded_on', 'series_a_date'], inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    \n",
    "    df.drop(columns=['status', 'roles', 'name', 'primary_role', 'male_founders_sum', 'female_founders_sum',\n",
    "                   'founders_country_code_nunique', 'state_code_angel', 'state_code_pre_seed',\n",
    "                   'state_code_seed', 'state_code_series_b', 'state_code'], inplace=True)\n",
    "    \n",
    "    df.rename(columns={'founder_uuid_count': 'number_of_founders',\n",
    "                        'male_founders_proportion': 'p_male',\n",
    "                        'country_code_entropy_first': 'nationality_entropy',\n",
    "                        'time_since_graduation_to_founding_min': 'tdelta_gradution_founding_min',\n",
    "                        'time_since_graduation_to_founding_max': 'tdelta_gradution_founding_max',\n",
    "                        'time_since_graduation_to_founding_mean': 'tdelta_gradution_founding_mean',\n",
    "                        'time_spent_studying_per_degree_mean': 't_studying_per_degree_mean',\n",
    "                        'time_spent_studying_mean': 't_studying_mean',\n",
    "                        'time_spent_studying_sum': 't_studying_sum',\n",
    "                        'number_of_degrees_completed_sum': 'degrees_completed',\n",
    "                        'number_of_degrees_started_sum': 'degrees_started',\n",
    "                        'business_experience_f_sum': 'business_experience',\n",
    "                        'coding_experience_f_sum': 'coding_experience',\n",
    "                        'raised_amount_usd_series_a': 'usd_a',\n",
    "                        'raised_amount_usd_seed': 'usd_seed',\n",
    "                        'raised_amount_usd_pre_seed': 'usd_pre_seed',\n",
    "                        'raised_amount_usd_angel': 'usd_angel',\n",
    "                        'investor_count_series_a': 'investors_a',\n",
    "                        'investor_count_seed': 'investors_seed',\n",
    "                        'investor_count_pre_seed': 'investors_pre_seed',\n",
    "                        'investor_count_angel': 'investors_angel',\n",
    "                        'announced_on_pre_seed': 'pre_seed',\n",
    "                        'announced_on_angel': 'angel',\n",
    "                        'announced_on_seed': 'seed',\n",
    "                        'announced_on_series_a': 'series_a_date',\n",
    "                        'announced_on_series_b': 'series_b_announced'}, inplace=True)\n",
    "                        \n",
    "    df['pre_seed'] = df['pre_seed'].notnull().astype(int)\n",
    "    df['angel'] = df['angel'].notnull().astype(int)\n",
    "    df['seed'] = df['seed'].notnull().astype(int)\n",
    "    df['series_b_announced'] = df['series_b_announced'].notnull().astype(int)\n",
    "\n",
    "    # introduce column acc_usd_till_series_a\n",
    "    df['acc_usd_pre_series_a'] = df['usd_angel'] + df['usd_pre_seed'] + df['usd_seed']\n",
    "\n",
    "    # introduce column acc_inv_till_series_a\n",
    "    df['acc_inv_pre_series_a'] = df['investors_angel'] + df['investors_pre_seed'] + df['investors_seed']\n",
    "\n",
    "    # now drop all columns that are not needed\n",
    "    df.drop(columns=['usd_angel', 'usd_pre_seed', 'usd_seed', 'investors_angel', 'investors_pre_seed', 'investors_seed'], inplace=True)\n",
    "\n",
    "    df['state_code_series_a'] = df['state_code_series_a'].fillna(test['country_code'])\n",
    "\n",
    "\n",
    "    df = categorize(df)\n",
    "    df = extract_dates(df)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_copy['country_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = feature_engineering(test_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_copy = final.copy()\n",
    "# delete companies with negative age_series_a\n",
    "final_copy = final_copy[final_copy['age_series_a'] > 0]\n",
    "print(len(final_copy))\n",
    "filtered_final = final_copy[final_copy[['state_code_series_a', 'country_code']].notna().all(axis=1)]\n",
    "print(len(filtered_final))\n",
    "# if t_studying_mean is NA, replace with t_studying_sum\n",
    "filtered_final['t_studying_mean'] = filtered_final['t_studying_mean'].fillna(filtered_final['t_studying_sum'])\n",
    "print(len(filtered_final))\n",
    "# if time between rounds is inf, replace it by calculating the time between founding and series a\n",
    "mask = filtered_final['time_between_rounds'] == np.inf\n",
    "filtered_final.loc[mask, 'time_between_rounds'] = filtered_final.loc[mask, 'age_series_a']\n",
    "print(len(filtered_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = filtered_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = final['state_code_series_a'].value_counts()\n",
    "states = states[states < 10]\n",
    "\n",
    "\n",
    "testal = final[~final['state_code_series_a'].isin(states.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = testal['country_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = testal['country_code'].value_counts()\n",
    "\n",
    "countries = countries[countries < 10]\n",
    "\n",
    "tester = testal[~testal['country_code'].isin(countries.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete negative t_studying_sum values, but impute NAs with 0\n",
    "tester['t_studying_sum'] = tester['t_studying_sum'].fillna(0)\n",
    "tester = tester[tester['t_studying_sum'] >= 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invent new column 'had_founders_information' which is 0 if number_of_founders is NA, 1 otherwise\n",
    "tester['had_founders_information'] = tester['number_of_founders'].notna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester = tester.copy()\n",
    "# create binary indicator variable showing whether there has been any study time\n",
    "tester['not_studied'] = (tester['t_studying_sum'] == 0).astype(int)\n",
    "\n",
    "# fill NAs with 0 for columns t_studying_per_degree_mean, tdelta_gradution_founding_min, \n",
    "# tdelta_gradution_founding_max, tdelta_gradution_founding_mean\n",
    "tester['t_studying_per_degree_mean'] = tester['t_studying_per_degree_mean'].fillna(0)\n",
    "tester['tdelta_gradution_founding_min'] = tester['tdelta_gradution_founding_min'].fillna(0)\n",
    "tester['tdelta_gradution_founding_max'] = tester['tdelta_gradution_founding_max'].fillna(0)\n",
    "tester['tdelta_gradution_founding_mean'] = tester['tdelta_gradution_founding_mean'].fillna(0)\n",
    "\n",
    "# fill NAs with 0 for columns business_experience, coding_experience\n",
    "tester['business_experience'] = tester['business_experience'].fillna(0)\n",
    "tester['coding_experience'] = tester['coding_experience'].fillna(0)\n",
    "# same for degrees_completed, degrees_started, number_of_founders, t_studying_mean, nationality_entropy, p_male\n",
    "tester['degrees_completed'] = tester['degrees_completed'].fillna(0)\n",
    "tester['degrees_started'] = tester['degrees_started'].fillna(0)\n",
    "# fill number of founders with column mean\n",
    "tester['number_of_founders'] = tester['number_of_founders'].fillna(tester['number_of_founders'].mean())\n",
    "tester['t_studying_mean'] = tester['t_studying_mean'].fillna(0)\n",
    "tester['nationality_entropy'] = tester['nationality_entropy'].fillna(tester['nationality_entropy'].mean())\n",
    "tester['p_male'] = tester['p_male'].fillna(tester['p_male'].mean())\n",
    "# invent variable money_per_investor_series_a\n",
    "tester['money_per_investor_series_a'] = tester['usd_a'] / tester['investors_a']\n",
    "\n",
    "# invent variable money_per_investor_pre_series_a\n",
    "tester['money_per_investor_pre_series_a'] = tester['acc_usd_pre_series_a'] / tester['acc_inv_pre_series_a']\n",
    "\n",
    "# in both columns, if value is inf or missing, replace with 0\n",
    "tester['money_per_investor_series_a'] = tester['money_per_investor_series_a'].replace([np.inf, -np.inf], 0)\n",
    "tester['money_per_investor_pre_series_a'] = tester['money_per_investor_pre_series_a'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "# also if it is missing\n",
    "tester['money_per_investor_series_a'] = tester['money_per_investor_series_a'].fillna(0)\n",
    "tester['money_per_investor_pre_series_a'] = tester['money_per_investor_pre_series_a'].fillna(0)\n",
    "\n",
    "tester.drop(columns=['series_b_announced'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tester_test = tester[tester['founded_year'] >= 1995]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_time_series_splits(data_length, tscv):\n",
    "\n",
    "    data = np.arange(data_length)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(tscv.split(data)):\n",
    "\n",
    "        offset = i * 0.1\n",
    "        \n",
    "        plt.plot(train_index, np.zeros_like(train_index) + offset, 'b', label='Training set' if i == 0 else \"\")\n",
    "\n",
    "        plt.plot(test_index, np.zeros_like(test_index) + offset, 'r', label='Testing set' if i == 0 else \"\")\n",
    "\n",
    "    plt.title('Time Series Split Visualization')\n",
    "    plt.xlabel('Data Index')\n",
    "    plt.ylabel('Fold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformer_cols = ['usd_a', 'age_series_a', 'money_per_investor_series_a', 'money_per_investor_pre_series_a', 'degrees_started', 'degrees_completed', 'number_of_founders',\n",
    "                        'investors_a', 't_studying_sum', 'business_experience', 'number_of_companies_founded_before_mean',\n",
    "                        'number_of_companies_founded_before_max', 'number_of_companies_founded_before_min', 'number_of_successful_companies_founded_before_mean']\n",
    "\n",
    "sqrt_transformer_cols = ['acc_usd_pre_series_a', 'time_between_rounds', 'acc_inv_pre_series_a', 't_studying_mean']\n",
    "\n",
    "normal_scale_cols = ['Media and Entertainment', 'Technology',\n",
    "       'Commerce and Business', 'Social and Community Services',\n",
    "       'Health and Life Sciences', 'Environmental and Sustainability',\n",
    "       'Design and Creativity', 'Transportation and Travel',\n",
    "       'Food and Beverage', 'Miscellaneous', 'Sports', 'Software',\n",
    "       'GovAndMilitary', 'Biotechnology', 'tdelta_gradution_founding_min', 't_studying_per_degree_mean', 'nationality_entropy',\n",
    "       'number_of_employees_max', 'number_of_funding_rounds', 'tdelta_gradution_founding_max', 'tdelta_gradution_founding_mean',\n",
    "       'p_male', 'number_of_successful_companies_founded_before_max', 'success_ratio_mean', 'success_ratio_max', 'degrees_per_founder_started',\n",
    "       'degrees_per_founder_completed']\n",
    "\n",
    "log_transformer_cols_prefixed = [\"log__\" + col for col in log_transformer_cols]\n",
    "sqrt_transformer_cols_prefixed = [\"sqrt__\" + col for col in sqrt_transformer_cols]\n",
    "normal_transformer_cols_prefixed = [\"remainder__\" + col for col in normal_scale_cols]\n",
    "\n",
    "sin_transformer_cols = ['founded_month',\n",
    "       'foundation_dayofyear', 'founded_quarter',\n",
    "       'series_a_month', 'series_a_dayofyear', 'series_a_quarter']\n",
    "\n",
    "dummy_cols = ['state_code_series_a', 'country_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_df = tester_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_df(modelling_df):\n",
    "    modelling_df['series_a_date'] = pd.to_datetime(modelling_df['series_a_dayofyear'].astype(str) + ' ' + modelling_df['series_a_year'].astype(str), format='%j %Y')\n",
    "\n",
    "    modelling_df.sort_values(by='series_a_date', ascending=True, inplace=True)\n",
    "\n",
    "    modelling_df.drop(columns=['series_a_date'], inplace=True)\n",
    "\n",
    "    return modelling_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_df = sort_df(modelling_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# revert not_studied to had_study_information \n",
    "test_column = modelling_df['not_studied'] - 1\n",
    "test_column = test_column.abs()\n",
    "\n",
    "modelling_df['had_study_information'] = test_column\n",
    "\n",
    "# impute degrees_per_founder_started and degrees_per_founder_completed with 0\n",
    "modelling_df['degrees_per_founder_started'] = modelling_df['degrees_per_founder_started'].fillna(0)\n",
    "modelling_df['degrees_per_founder_completed'] = modelling_df['degrees_per_founder_completed'].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove column org_uuid\n",
    "modelling_df.drop(columns=['org_uuid', 'not_studied'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_df.to_csv('modelling_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_df = pd.read_csv('modelling_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eda_df = modelling_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the occurence of industry per founding year\n",
    "occ_technology = eda_df.groupby(['founded_year'])['Technology'].sum()\n",
    "occ_media = eda_df.groupby(['founded_year'])['Media and Entertainment'].sum()\n",
    "occ_commerce = eda_df.groupby(['founded_year'])['Commerce and Business'].sum()\n",
    "occ_social = eda_df.groupby(['founded_year'])['Social and Community Services'].sum()\n",
    "occ_health = eda_df.groupby(['founded_year'])['Health and Life Sciences'].sum()\n",
    "occ_environment = eda_df.groupby(['founded_year'])['Environmental and Sustainability'].sum()\n",
    "occ_design = eda_df.groupby(['founded_year'])['Design and Creativity'].sum()\n",
    "occ_transport = eda_df.groupby(['founded_year'])['Transportation and Travel'].sum()\n",
    "occ_food = eda_df.groupby(['founded_year'])['Food and Beverage'].sum()\n",
    "occ_misc = eda_df.groupby(['founded_year'])['Miscellaneous'].sum()\n",
    "occ_sports = eda_df.groupby(['founded_year'])['Sports'].sum()\n",
    "occ_software = eda_df.groupby(['founded_year'])['Software'].sum()\n",
    "occ_gov = eda_df.groupby(['founded_year'])['GovAndMilitary'].sum()\n",
    "occ_bio = eda_df.groupby(['founded_year'])['Biotechnology'].sum()\n",
    "\n",
    "# plot the occurence of industry per founding year\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(occ_software, label='Software')\n",
    "plt.plot(occ_technology, label='Technology')\n",
    "plt.plot(occ_commerce, label='Commerce and Business')\n",
    "plt.plot(occ_social, label='Social and Community Services')\n",
    "plt.plot(occ_health, label='Health and Life Sciences')\n",
    "plt.plot(occ_environment, label='Environmental and Sustainability')\n",
    "plt.plot(occ_design, label='Design and Creativity')\n",
    "plt.plot(occ_transport, label='Transportation and Travel')\n",
    "plt.plot(occ_food, label='Food and Beverage')\n",
    "plt.plot(occ_misc, label='Miscellaneous')\n",
    "plt.plot(occ_sports, label='Sports', linestyle='dashed')\n",
    "plt.plot(occ_gov, label='GovAndMilitary')\n",
    "plt.plot(occ_bio, label='Biotechnology')\n",
    "plt.plot(occ_media, label='Media and Entertainment', linestyle='dashed')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Companies')\n",
    "plt.title('Number of Companies per Industry per Year')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rate(df):\n",
    "    success_rate = df.groupby('series_a_year')['successful'].mean()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    success_rate.plot(kind='bar')\n",
    "    plt.title('Success Rate Over the Years')\n",
    "    plt.xlabel('Series A Year')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('success_rate.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_success_rate(eda_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rate(df):\n",
    "    # Group by year and calculate the number of companies and success rate\n",
    "    group_by_year = df.groupby('series_a_year').agg(\n",
    "        num_companies=('successful', 'size'),\n",
    "        success_rate=('successful', 'mean')\n",
    "    ).reset_index()\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "    # Bar plot for the number of companies\n",
    "    ax1.bar(group_by_year['series_a_year'], group_by_year['num_companies'], color='#1f77b4', label='Number of Companies')\n",
    "    ax1.set_xlabel('Series A Year')\n",
    "    ax1.set_ylabel('Number of Companies', color='#1f77b4')\n",
    "    ax1.tick_params(axis='y', labelcolor='#1f77b4')\n",
    "\n",
    "    # Line plot for the success rate\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(group_by_year['series_a_year'], group_by_year['success_rate'], color='firebrick', marker='o', label='Success Rate')\n",
    "    ax2.set_ylabel('Success Rate', color='firebrick')\n",
    "    ax2.tick_params(axis='y', labelcolor='firebrick')\n",
    "    # set limits from 0 to 1\n",
    "    ax2.set_ylim(0, 1)\n",
    "\n",
    "    # Title and layout\n",
    "    plt.title('Number of Companies and Success Rate Over the Years')\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # Show plot\n",
    "    plt.savefig('success_rate_and_num_companies.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_success_rate(eda_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_success_rate2(df):\n",
    "    success_rate = df.groupby('series_a_year')['successful'].mean()\n",
    "    number_of_companies = df['series_a_year'].value_counts().sort_index()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    success_rate.plot(kind='bar', color='blue', alpha=0.7, label='Success Rate')\n",
    "    number_of_companies.plot(kind='line', color='red', label='Number of Companies')\n",
    "    plt.title('Success Rate and Number of Companies Over the Years')\n",
    "    plt.xlabel('Series A Year')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('success_rate.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = [\"number_of_employees_max\", \n",
    "\"number_of_companies_founded_before_mean\", \n",
    "\"number_of_companies_founded_before_max\", \n",
    "\"number_of_companies_founded_before_min\", \n",
    "\"number_of_successful_companies_founded_before_mean\", \n",
    "\"number_of_successful_companies_founded_before_max\", \n",
    "\"success_ratio_mean\", \n",
    "\"success_ratio_max\", \n",
    "\"coding_experience\", \n",
    "\"business_experience\", \n",
    "\"degrees_started\", \n",
    "\"degrees_completed\", \n",
    "\"t_studying_sum\", \n",
    "\"t_studying_mean\", \n",
    "\"t_studying_per_degree_mean\", \n",
    "\"tdelta_gradution_founding_mean\", \n",
    "\"tdelta_gradution_founding_max\", \n",
    "\"tdelta_gradution_founding_min\", \n",
    "\"number_of_founders\", \n",
    "\"nationality_entropy\", \n",
    "\"p_male\", \n",
    "\"degrees_per_founder_started\", \n",
    "\"degrees_per_founder_completed\", \n",
    "\"acc_usd_pre_series_a\", \n",
    "\"acc_inv_pre_series_a\", \n",
    "\"founded_year\", \n",
    "\"series_a_year\", \n",
    "\"age_series_a\", \n",
    "\"number_of_funding_rounds\", \n",
    "\"time_between_rounds\", \n",
    "\"had_study_information\"\n",
    "]\n",
    "\n",
    "corr = eda_df[numerical_cols].corr()\n",
    "\n",
    "# plot the heatmap\n",
    "plt.figure(figsize=(24, 16))\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title('Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig('correlation_matrix.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_countries = eda_df['country_code'].value_counts().nlargest(10).index\n",
    "sns.countplot(data=eda_df, x='country_code', order=top_10_countries)\n",
    "plt.xlabel('Country')\n",
    "plt.ylabel('Number of Organizations')\n",
    "plt.title('Number of Organizations by Country (Top 10)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_columns = [\n",
    "    'founded_quarter', 'foundation_dayofyear', 'founded_month', 'founded_year',\n",
    "    'series_a_quarter', 'series_a_dayofyear', 'series_a_month', 'series_a_year'\n",
    "]\n",
    "\n",
    "date_df = eda_df[date_columns]\n",
    "\n",
    "date_df.hist(bins=25, figsize=(20, 15))\n",
    "plt.suptitle('Histograms of Date Variables', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96]) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = ['Media and Entertainment', 'Technology',\n",
    "       'Commerce and Business', 'Social and Community Services',\n",
    "       'Health and Life Sciences', 'Environmental and Sustainability',\n",
    "       'Design and Creativity', 'Transportation and Travel',\n",
    "       'Food and Beverage', 'Miscellaneous', 'Sports', 'Software',\n",
    "       'GovAndMilitary', 'Biotechnology']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the VIFs of all \n",
    "numerical_cols = eda_df.select_dtypes(include=np.number).columns\n",
    "\n",
    "numerical_cols = [col for col in numerical_cols if col not in dummies]\n",
    "\n",
    "# exclude the categorical columns\n",
    "X = add_constant(eda_df[numerical_cols])\n",
    "\n",
    "VIFS = pd.Series([variance_inflation_factor(X.values, i) \n",
    "               for i in range(X.shape[1])], \n",
    "              index=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = modelling_df.drop(columns=['successful'])\n",
    "y = modelling_df['successful']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.18055, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote, y_train_smote = X_train.copy(), y_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Final pipeline ####\n",
    "log_transformer = FunctionTransformer(np.log1p, validate=False)  \n",
    "sqrt_transformer = FunctionTransformer(np.sqrt, validate=False)\n",
    "poly_transformer = PolynomialFeatures(degree=2)\n",
    "sin_transformer = FunctionTransformer(np.sin, validate=False)\n",
    "\n",
    "scaler_log = StandardScaler()\n",
    "scaler_sqrt = StandardScaler()\n",
    "normal_scaler = StandardScaler()\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# First column transformer for transformations and imputation\n",
    "preprocessor_transformations = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('target', TargetEncoder(handle_unknown='ignore'), dummy_cols),\n",
    "        ('log', Pipeline([('imputer', imputer), ('transform', log_transformer)]), log_transformer_cols),\n",
    "        ('sqrt', Pipeline([('imputer', imputer), ('transform', sqrt_transformer)]), sqrt_transformer_cols),\n",
    "        ('sin', sin_transformer, sin_transformer_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "preprocessor_transformations.set_output(transform='pandas')\n",
    "\n",
    "preprocessor_scaling = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale_sqrt', scaler_sqrt, sqrt_transformer_cols_prefixed),\n",
    "        ('scale_log', scaler_log, log_transformer_cols_prefixed),\n",
    "        ('normal_scale', Pipeline([('imputer', imputer), ('scaler', normal_scaler)]), normal_transformer_cols_prefixed)\n",
    "    ],\n",
    "    remainder='passthrough'  # Include columns that are not specified without any transformations\n",
    ")\n",
    "\n",
    "preprocessor_scaling.set_output(transform='pandas')\n",
    "\n",
    "def apply_preprocessor(X, y):\n",
    "    X_rea = preprocessor_transformations.fit_transform(X, y)\n",
    "    #X_re = preprocessor_scaling.fit_transform(X_rea, y)\n",
    "    return X_rea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boosting_model(X_train_smote, y_train_smote, tscv, hyperparameters, SMOTEE):\n",
    "\n",
    "    SMOTER = SMOTE(sampling_strategy=1.0, random_state=808)\n",
    "\n",
    "    results = {}\n",
    "\n",
    "    for i, (train_index, test_index) in tqdm(enumerate(tscv.split(X_train_smote))):\n",
    "        \n",
    "        X_training = X_train_smote.iloc[train_index]\n",
    "        y_training = y_train_smote.iloc[train_index]\n",
    "\n",
    "        X_validation = X_train_smote.iloc[test_index]\n",
    "        y_validation = y_train_smote.iloc[test_index]\n",
    "\n",
    "        # Apply preprocessors to the training set of each fold\n",
    "        X_train_transformed = apply_preprocessor(X_training, y_training)\n",
    "        if SMOTEE:\n",
    "            # print the proportion of the minority class before SMOTE\n",
    "            positive_class_proportion = y_training.sum() / len(y_training)\n",
    "            print(f\"Before: {positive_class_proportion}\")\n",
    "            X_train_transformed, y_train_transformed = SMOTER.fit_resample(X_train_transformed, y_training)\n",
    "            # print the proportion of the minority class after SMOTE\n",
    "            positive_class_proportion = y_train_transformed.sum() / len(y_train_transformed)\n",
    "            print(f\"After: {positive_class_proportion}\")\n",
    "        else:\n",
    "            y_train_transformed = y_training  # Assuming no transformation is applied to the labels\n",
    "\n",
    "        # Apply preprocessors to the validation set of each fold\n",
    "        X_validation_transformed = apply_preprocessor(X_validation, y_validation)\n",
    "        y_validation_transformed = y_validation  # Assuming no transformation is applied to the labels\n",
    "\n",
    "        # now fit the model with different hyperparameters manually for each combination and store the results \n",
    "        # lets add a progress bar\n",
    "        for n_estimators in hyperparameters['tree__n_estimators']:\n",
    "            print(f'Fold {i + 1}, n_estimators: {n_estimators}')\n",
    "            for max_depth in hyperparameters['tree__max_depth']:\n",
    "                print(f'Fold {i + 1}, n_estimators: {n_estimators}, max_depth: {max_depth}')\n",
    "                for learning_rate in hyperparameters['tree__learning_rate']:\n",
    "                    print(f'Fold {i + 1}, n_estimators: {n_estimators}, max_depth: {max_depth}, learning_rate: {learning_rate}')\n",
    "                    for max_features in hyperparameters['tree__max_features']:\n",
    "                        print(f'Fold {i + 1}, n_estimators: {n_estimators}, max_depth: {max_depth}, learning_rate: {learning_rate}, max_features: {max_features}')\n",
    "                        model = GradientBoostingClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, \n",
    "                                                           max_features=max_features, random_state=808)\n",
    "                        model.fit(X_train_transformed, y_train_transformed)\n",
    "                        y_pred = model.predict(X_validation_transformed)\n",
    "                        results[(i, n_estimators, max_depth, learning_rate, max_features)] = {\n",
    "                            'accuracy': accuracy_score(y_validation_transformed, y_pred),\n",
    "                            'precision': precision_score(y_validation_transformed, y_pred),\n",
    "                            'recall': recall_score(y_validation_transformed, y_pred),\n",
    "                            'f1': f1_score(y_validation_transformed, y_pred),\n",
    "                            'roc_auc': roc_auc_score(y_validation_transformed, y_pred)\n",
    "                        }\n",
    "                    \n",
    "    \"\"\"X_train_transformed = apply_preprocessor(X_train_smote, y_train_smote)\n",
    "    y_train_transformed = y_train_smote\n",
    "\n",
    "    if SMOTEE:\n",
    "        X_train_transformed, y_train_transformed = SMOTER.fit_resample(X_train_transformed, y_train_smote)\"\"\"\n",
    "\n",
    "    results_df = pd.DataFrame(results).T\n",
    "    results_df['f1'] = results_df['f1'].astype(float)\n",
    "    results_df['accuracy'] = results_df['accuracy'].astype(float)\n",
    "    results_df['precision'] = results_df['precision'].astype(float)\n",
    "\n",
    "    results_dft = results_df.reset_index()\n",
    "\n",
    "    results_dft.rename(columns={'level_0': 'fold', 'level_1': 'n_estimators', 'level_2': 'max_depth', \n",
    "                                'level_3': 'learning_rate', 'level_4': 'max_features'}, inplace=True)\n",
    "\n",
    "    return results_dft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets build a function that does all the above steps\n",
    "def cv_score_plus_exponential(results_df):\n",
    "\n",
    "    results_dft = results_df.groupby(['n_estimators', 'max_depth', 'learning_rate', 'max_features']).agg({\n",
    "            'accuracy': 'mean',\n",
    "            'precision': 'mean',\n",
    "            'recall': 'mean',\n",
    "            'f1': 'mean',\n",
    "            'roc_auc': 'mean'\n",
    "        }).sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "    base = 1.2\n",
    "    results_df['weights'] = base ** results_df['fold']\n",
    "    \n",
    "    # Group by and apply weighted aggregation\n",
    "    def weighted_avg(group, col):\n",
    "        return np.average(group[col], weights=group['weights'])\n",
    "    \n",
    "    results_dft_exponential = results_df.groupby(['n_estimators', 'max_depth', 'learning_rate', 'max_features']).apply(\n",
    "        lambda group: pd.Series({\n",
    "            'accuracy': weighted_avg(group, 'accuracy'),\n",
    "            'precision': weighted_avg(group, 'precision'),\n",
    "            'recall': weighted_avg(group, 'recall'),\n",
    "            'f1': weighted_avg(group, 'f1'),\n",
    "            'roc_auc': weighted_avg(group, 'roc_auc')\n",
    "        })\n",
    "    ).sort_values(by='accuracy', ascending=False)\n",
    "\n",
    "    return results_dft, results_dft_exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_hyperparameters = {\n",
    "    'tree__n_estimators': [100],\n",
    "    'tree__max_depth': [3],\n",
    "    'tree__learning_rate': [0.1],\n",
    "    'tree__max_features': ['sqrt']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'tree__n_estimators': [500, 1000],\n",
    "    'tree__max_depth': [5, 8, 15],\n",
    "    'tree__learning_rate': [0.001, 0.01, 1],\n",
    "    'tree__max_features': [1.0, 'sqrt', 'log2', None]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expanding Window with Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tscv_eg = TimeSeriesSplit(n_splits=6, test_size=3000, gap=1000)\n",
    "\n",
    "plot_time_series_splits(23000, tscv_eg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_expanding_gap_no_smote = boosting_model(X_train_smote, y_train_smote, tscv_eg, hyperparameters, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "results_expanding_gap_no_smote.to_csv('results_expanding_gap_no_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_expanding_gap_smote = boosting_model(X_train_smote, y_train_smote, tscv_eg, hyperparameters, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "results_expanding_gap_smote.to_csv('results_expanding_gap_smote.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranking results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in expanding fold yes gap yes SMOTE\n",
    "results_expanding_yes_gap_yes_smote = pd.read_csv('results_expanding_gap_smote.csv')\n",
    "results_expanding_yes_gap_yes_smote_df, results_expanding_yes_gap_yes_smote_exponential_df = cv_score_plus_exponential(results_expanding_yes_gap_yes_smote)\n",
    "\n",
    "# read in expanding fold yes gap no SMOTE\n",
    "results_expanding_yes_gap_no_smote = pd.read_csv('results_expanding_gap_no_smote.csv')\n",
    "results_expanding_yes_gap_no_smote_df, results_expanding_yes_gap_no_smote_exponential_df = cv_score_plus_exponential(results_expanding_yes_gap_no_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = apply_preprocessor(X_train_smote, y_train_smote)\n",
    "\n",
    "# now fit the model with the best hyperparameters\n",
    "model = GradientBoostingClassifier(n_estimators=1000, max_depth=5, learning_rate=0.01, max_features='log2', random_state=808)\n",
    "\n",
    "model.fit(X_train_transformed, y_train_smote)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_copy = X_test.copy()\n",
    "\n",
    "X_test_copy = apply_preprocessor(X_test_copy, y_test)\n",
    "\n",
    "X_test_copy.columns = X_test_copy.columns.str.replace('remainder__', '')\n",
    "X_test_copy.columns = X_test_copy.columns.str.replace('log__', '')\n",
    "X_test_copy.columns = X_test_copy.columns.str.replace('sqrt__', '')\n",
    "\n",
    "indices = [X_test_copy.columns.get_loc(col) for col in ['number_of_employees_max', 'usd_a', 'investors_a', 'money_per_investor_series_a', 'age_series_a', 'time_between_rounds', 'founded_year', 'number_of_founders', 't_studying_sum', 'degrees_per_founder_started']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.Explainer(model)\n",
    "\n",
    "X_test_transformed = apply_preprocessor(X_test, y_test)\n",
    "\n",
    "shap_values = explainer.shap_values(X_test_transformed)\n",
    "\n",
    "X_test_transformed_copy = X_test_transformed.copy()\n",
    "\n",
    "X_test_transformed_copy.columns = X_test_transformed_copy.columns.str.replace('remainder__', '')\n",
    "X_test_transformed_copy.columns = X_test_transformed_copy.columns.str.replace('log__', '')\n",
    "X_test_transformed_copy.columns = X_test_transformed_copy.columns.str.replace('sqrt__', '')\n",
    "\n",
    "shap_values = shap_values[:, indices]\n",
    "\n",
    "X_test_transformed_copy = X_test_transformed_copy.iloc[:, indices]\n",
    "\n",
    "shap.summary_plot(shap_values, X_test_transformed_copy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = apply_preprocessor(X_test, y_test)\n",
    "\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_importances = X_train_transformed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_importances.columns = [col.replace('log__', '').replace('sqrt__', '').replace('remainder__', '').replace('sin__', '').replace('target__', '') for col in X_importances.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a df with two columns - one holds the feature names, the other the feature importances\n",
    "feature_importance_df = pd.DataFrame({'feature': X_importances.columns, 'importance': feature_importance})\n",
    "\n",
    "feature_importance_df.sort_values(by='importance', ascending=False, inplace=True)\n",
    "\n",
    "# add up all importances of the different categories into one\n",
    "categories = ['Media and Entertainment', 'Technology', 'Commerce and Business', 'Social and Community Services',\n",
    "       'Health and Life Sciences', 'Environmental and Sustainability',\n",
    "       'Design and Creativity', 'Transportation and Travel',\n",
    "       'Food and Beverage', 'Miscellaneous', 'Sports', 'Software',\n",
    "       'GovAndMilitary', 'Biotechnology']\n",
    "\n",
    "category_importances = feature_importance_df[feature_importance_df['feature'].isin(categories)]['importance'].sum()\n",
    "\n",
    "# now delete the individual importances\n",
    "feature_importance_df = feature_importance_df[~feature_importance_df['feature'].isin(categories)]\n",
    "\n",
    "# add the category importances as a row to the df\n",
    "category_importance_df = pd.DataFrame({'feature': 'industry', 'importance': category_importances}, index=[0])\n",
    "\n",
    "feature_importance_df = pd.concat([feature_importance_df, category_importance_df])\n",
    "\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# plot the feature importances\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.barplot(data=feature_importance_df, x='importance', y='feature', color='#4878CF')\n",
    "plt.title('Feature Importances')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('feature_importances.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Among previous funding rounds, organizational factors, and founding team background, which area has the largest impact on predictions? Which the lowest?\n",
    "# To answer the above research question, the previous variable importances have to be aggregated into the three categories (or other)\n",
    "\n",
    "funding_rounds_vars = ['usd_a', 'state_code_series_a', 'series_a_year', 'investors_a', 'money_per_investor_series_a', 'time_between_rounds', 'series_a_dayofyear',\n",
    "                       'acc_inv_pre_series_a', 'money_per_investor_pre_series_a', 'acc_usd_pre_series_a', 'angel', 'number_of_funding_rounds', 'series_a_month', \n",
    "                       'series_a_quarter', 'seed', 'pre_seed']\n",
    "\n",
    "organizational_vars = ['number_of_employees_max', 'industry', 'age_series_a', 'country_code', 'founded_year', 'foundation_dayofyear', 'founded_month', 'founded_quarter']\n",
    "\n",
    "founding_team_vars = ['number_of_founders', 't_studying_sum', 'degrees_per_founder_started', 'p_male', 't_studying_mean', 'degrees_started', 'degrees_per_founder_completed',\n",
    "                      'had_founders_information', 't_studying_per_degree_mean', 'tdelta_gradution_founding_min', 'tdelta_gradution_founding_max', 'tdelta_gradution_founding_mean',\n",
    "                      'nationality_entropy', 'degrees_completed', 'had_study_information', 'number_of_companies_founded_before_mean', 'coding_experience', \n",
    "                      'number_of_companies_founded_before_max', 'business_experience', 'number_of_successful_companies_founded_before_mean', 'success_ratio_max',\n",
    "                      'success_ratio_mean', 'number_of_companies_founded_before_min', 'number_of_successful_companies_founded_before_max']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_vars = ['t_studying_sum', 'degrees_per_founder_started', 't_studying_mean', 'degrees_started', 'degrees_per_founder_completed', 't_studying_per_degree_mean', \n",
    "                       'tdelta_gradution_founding_min', 'tdelta_gradution_founding_max', 'tdelta_gradution_founding_mean', 'degrees_completed', 'had_study_information',\n",
    "                       'coding_experience', 'business_experience']\n",
    "\n",
    "team_composition_vars = ['number_of_founders', 'p_male', 'had_founders_information', 'nationality_entropy']\n",
    "\n",
    "professional_experience_vars = ['number_of_companies_founded_before_mean', 'number_of_companies_founded_before_max', 'number_of_successful_companies_founded_before_min',\n",
    "                                'number_of_successful_companies_founded_before_max', 'success_ratio_mean', 'success_ratio_max', 'number_of_companies_founded_before_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_vars = ['founded_year', 'foundation_dayofyear', 'founded_month', 'founded_quarter']\n",
    "\n",
    "company_stats_vars = ['number_of_employees_max', 'industry', 'age_series_a', 'country_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_vars = ['usd_a', 'investors_a', 'money_per_investor_series_a', 'acc_inv_pre_series_a', 'money_per_investor_pre_series_a', 'acc_usd_pre_series_a']\n",
    "\n",
    "time_vars = ['time_between_rounds', 'series_a_dayofyear', 'series_a_month', 'series_a_quarter', 'series_a_year']\n",
    "\n",
    "basic_vars = ['state_code_series_a', 'angel', 'seed', 'pre_seed', 'number_of_funding_rounds']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate importances for organizational information subcategories\n",
    "date_importance = feature_importance_df[feature_importance_df['feature'].isin(date_vars)]['importance'].sum()\n",
    "company_stats_importance = feature_importance_df[feature_importance_df['feature'].isin(company_stats_vars)]['importance'].sum()\n",
    "organizational_importance = date_importance + company_stats_importance\n",
    "\n",
    "# Calculate importances for funding rounds subcategories\n",
    "money_importance = feature_importance_df[feature_importance_df['feature'].isin(money_vars)]['importance'].sum()\n",
    "time_importance = feature_importance_df[feature_importance_df['feature'].isin(time_vars)]['importance'].sum()\n",
    "basic_importance = feature_importance_df[feature_importance_df['feature'].isin(basic_vars)]['importance'].sum()\n",
    "funding_rounds_importance = money_importance + time_importance + basic_importance\n",
    "\n",
    "# Calculate importances for founding team subcategories\n",
    "degree_importance = feature_importance_df[feature_importance_df['feature'].isin(degree_vars)]['importance'].sum()\n",
    "team_composition_importance = feature_importance_df[feature_importance_df['feature'].isin(team_composition_vars)]['importance'].sum()\n",
    "professional_experience_importance = feature_importance_df[feature_importance_df['feature'].isin(professional_experience_vars)]['importance'].sum()\n",
    "founding_team_importance = degree_importance + team_composition_importance + professional_experience_importance\n",
    "\n",
    "# Create DataFrame for plotting\n",
    "categories = ['Funding Rounds', 'Organizational', 'Founding Team']\n",
    "importances = [funding_rounds_importance, organizational_importance, founding_team_importance]\n",
    "\n",
    "# Create subcategory lists for stacked bar plot\n",
    "funding_subcategories = [money_importance, time_importance, basic_importance]\n",
    "organizational_subcategories = [date_importance, company_stats_importance]\n",
    "founding_team_subcategories = [degree_importance, team_composition_importance, professional_experience_importance]\n",
    "\n",
    "# Define labels for the legend\n",
    "funding_labels = ['Investment Amount', 'Timing of Rounds', 'Funding Round Basics']\n",
    "organizational_labels = ['Founding Dates', 'Company Characteristics']\n",
    "founding_team_labels = ['Educational Background', 'Team Demographics', 'Entrepreneurial Experience']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(12, 8))\n",
    "bars = plt.bar(categories, importances, color='gray', edgecolor='black')\n",
    "\n",
    "# Adding subcategories to the Funding Rounds bar\n",
    "bottom_funding = 0\n",
    "funding_colors = sns.color_palette('rocket', len(funding_subcategories))\n",
    "for i, (sub_importance, label) in enumerate(zip(funding_subcategories, funding_labels)):\n",
    "    plt.bar('Funding Rounds', sub_importance, bottom=bottom_funding, label=label, color=funding_colors[i], edgecolor='black')\n",
    "    bottom_funding += sub_importance\n",
    "\n",
    "# Adding subcategories to the Organizational bar\n",
    "bottom_organizational = 0\n",
    "organizational_colors = sns.color_palette('mako', len(organizational_subcategories))\n",
    "for i, (sub_importance, label) in enumerate(zip(organizational_subcategories, organizational_labels)):\n",
    "    plt.bar('Organizational', sub_importance, bottom=bottom_organizational, label=label, color=organizational_colors[i], edgecolor='black')\n",
    "    bottom_organizational += sub_importance\n",
    "\n",
    "# Adding subcategories to the Founding Team bar\n",
    "bottom_founding = 0\n",
    "founding_colors = sns.color_palette('ch:s=-.2,r=.6', len(founding_team_subcategories))\n",
    "for i, (sub_importance, label) in enumerate(zip(founding_team_subcategories, founding_team_labels)):\n",
    "    plt.bar('Founding Team', sub_importance, bottom=bottom_founding, label=label, color=founding_colors[i], edgecolor='black')\n",
    "    bottom_founding += sub_importance\n",
    "\n",
    "# Customize plot\n",
    "plt.title('Variable Importances by Category')\n",
    "plt.ylabel('Importance')\n",
    "plt.legend(title='Subcategories')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('variable_importances_by_category.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reliability score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reliability_df = pd.read_csv('modelling_df.csv')\n",
    "\n",
    "normal_cols = [col for col in reliability_df.columns if col not in ['state_code_series_a', 'country_code', 'successful']]\n",
    "\n",
    "normal_cols = [\"remainder__\" + col for col in normal_cols if col not in sin_transformer_cols]\n",
    "\n",
    "target_cols = ['target__state_code_series_a', 'target__country_code']\n",
    "\n",
    "sin_cols = [\"sin__\" + col for col in sin_transformer_cols]\n",
    "y, X = reliability_df['successful'], reliability_df.drop(columns=['successful'])\n",
    "\n",
    "X_NN, X_reliability_obs, y_train_reliability, y_reliability_obs = train_test_split(X, y, test_size=0.18055, shuffle=False)\n",
    "\n",
    "sin_transformer = FunctionTransformer(np.sin, validate=False)\n",
    "normal_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "sin_scaler = StandardScaler()\n",
    "# First column transformer for transformations and imputation\n",
    "preprocessor_transformations_reliability = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('target', TargetEncoder(handle_unknown='ignore'), dummy_cols),\n",
    "        ('sin', sin_transformer, sin_transformer_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_transformations_reliability.set_output(transform='pandas')\n",
    "\n",
    "preprocessor_scaling_reliability = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale_all', normal_scaler, normal_cols),\n",
    "        ('scale_target', target_scaler, target_cols),\n",
    "        ('scale_sin', sin_scaler, sin_cols)\n",
    "    ],\n",
    "    remainder='passthrough' \n",
    ")\n",
    "\n",
    "preprocessor_scaling_reliability.set_output(transform='pandas')\n",
    "\n",
    "def apply_preprocessor_reliability(X, y):\n",
    "    X_rea = preprocessor_transformations_reliability.fit_transform(X, y)\n",
    "    X_re = preprocessor_scaling_reliability.fit_transform(X_rea, y)\n",
    "    return X_re\n",
    "feature_space = apply_preprocessor_reliability(X_NN, y_train_reliability)\n",
    "X_reliability_obs = apply_preprocessor_reliability(X_reliability_obs, y_reliability_obs)\n",
    "X_reliability_obs['is_test'] = 1\n",
    "feature_space['is_test'] = 0\n",
    "feature_space2 = pd.concat([feature_space, X_reliability_obs])\n",
    "\n",
    "k = 10\n",
    "knn = NearestNeighbors(n_neighbors=k, metric='euclidean')\n",
    "\n",
    "knn.fit(feature_space)\n",
    "\n",
    "distances, indices = knn.kneighbors(feature_space2)\n",
    "\n",
    "certainties = []\n",
    "\n",
    "for i in range(len(indices)):\n",
    "    certainty = y_train_reliability.iloc[indices[i]].mean()\n",
    "    if certainty < 0.5:\n",
    "        certainty = (1 - certainty)\n",
    "    certainties.append(y_train_reliability.iloc[indices[i]].mean())\n",
    "\n",
    "mean_distances = []\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    mean_distances.append(distances[i].mean())\n",
    "\n",
    "median_distances = []\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    median_distances.append(np.median(distances[i]))\n",
    "\n",
    "distance_sums = []    \n",
    "\n",
    "for i in range(len(distances)):\n",
    "    distance_sums.append(distances[i].sum())\n",
    "\n",
    "# lets build reliability_df that for each observation of the test set holds the certainties, mean_distances, median_distances and distance_sums\n",
    "reliability_measures = pd.DataFrame({'certainties': certainties, 'mean_distances': mean_distances, 'median_distances': median_distances, \n",
    "                                     'distance_sums': distance_sums, 'is_test': feature_space2['is_test']})\n",
    "\n",
    "reliability_df = reliability_measures.copy()\n",
    "\n",
    "# discard the last 2068 observations\n",
    "reliability_df = reliability_df[reliability_df['is_test'] == 1]\n",
    "reliability_df.drop(columns=['is_test'], inplace=True)\n",
    "\n",
    "reliability_df['certainties_percentiles'] = reliability_df['certainties'].rank(pct=True)\n",
    "reliability_df['mean_distances_percentiles'] = reliability_df['mean_distances'].rank(pct=True)\n",
    "reliability_df['median_distances_percentiles'] = reliability_df['median_distances'].rank(pct=True)\n",
    "reliability_df['distance_sums_percentiles'] = reliability_df['distance_sums'].rank(pct=True)\n",
    "\n",
    "reliability_df.drop(columns=['certainties', 'mean_distances', 'median_distances', 'distance_sums'], inplace=True)\n",
    "\n",
    "preprocessor_log = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('log', FunctionTransformer(np.log1p, validate=False), reliability_df.columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_log.set_output(transform='pandas')\n",
    "\n",
    "reliability_df1 = preprocessor_log.fit_transform(reliability_df)\n",
    "\n",
    "preprocessor_poly = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('poly', PolynomialFeatures(degree=2), reliability_df1.columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_poly.set_output(transform='pandas')\n",
    "\n",
    "reliability_df2 = preprocessor_poly.fit_transform(reliability_df1)\n",
    "\n",
    "preprocessor_standard = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('scale', StandardScaler(), reliability_df2.columns)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "preprocessor_standard.set_output(transform='pandas')\n",
    "\n",
    "reliability_final = preprocessor_standard.fit_transform(reliability_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_final['residual'] = (y_reliability_obs - y_pred).abs()\n",
    "\n",
    "X_train_reliability, X_test_reliability, y_train_reliability, y_test_reliability = train_test_split(reliability_final.drop(columns=['residual']), reliability_final['residual'], test_size=0.40805, random_state=808)\n",
    "\n",
    "SMOTER = SMOTE(sampling_strategy=1.0, random_state=808)\n",
    "\n",
    "X_train_reliability_smote, y_train_reliability_smote = SMOTER.fit_resample(X_train_reliability, y_train_reliability)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=808, max_iter=1000000)\n",
    "\n",
    "log_reg.fit(X_train_reliability_smote, y_train_reliability_smote)\n",
    "\n",
    "y_pred_reliability = log_reg.intercept_ + np.dot(X_test_reliability, log_reg.coef_.T)\n",
    "\n",
    "X_test_reliability['y_pred_reliability'] = 1 / (1 + np.exp(-y_pred_reliability))\n",
    "\n",
    "bins = [0, 0.2, 0.4, 0.6, 0.8, 1]\n",
    "labels = ['blue', 'green', 'yellow', 'orange', 'red']\n",
    "\n",
    "X_test_reliability['percentile_prediction'] = X_test_reliability['y_pred_reliability'].rank(pct=True)\n",
    "\n",
    "X_test_reliability['color'] = pd.cut(X_test_reliability['percentile_prediction'], bins=bins, labels=labels)\n",
    "\n",
    "X_test_reliability['is_residual'] = y_test_reliability\n",
    "\n",
    "prediction_certainties_final = X_test_reliability[['color', 'is_residual']]\n",
    "\n",
    "cross_tab = pd.crosstab(prediction_certainties_final['color'], prediction_certainties_final['is_residual'], normalize='index')\n",
    "cross_tab = cross_tab.reindex(labels[::-1])\n",
    "ax = cross_tab.plot(kind='bar', stacked=False)\n",
    "\n",
    "ax.legend(loc='upper right', title='is_residual', bbox_to_anchor=(1.1, 1))\n",
    "\n",
    "plt.title('Proportion of Prediction Reliability by Residual')\n",
    "plt.xlabel('Prediction Reliability')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets access the weights\n",
    "weights = log_reg.coef_\n",
    "\n",
    "# connect the weights to the feature names\n",
    "feature_names = reliability_final.columns\n",
    "\n",
    "# remove residual\n",
    "feature_names = feature_names.drop('residual')\n",
    "\n",
    "# create a df with the feature names and the weights\n",
    "feature_weights = pd.DataFrame({'feature': feature_names, 'weight': weights[0]})\n",
    "\n",
    "# sort the df by the weights\n",
    "feature_weights = feature_weights.sort_values(by='weight', ascending=False)\n",
    "\n",
    "# plot the feature weights\n",
    "plt.figure(figsize=(20, 12))\n",
    "sns.barplot(data=feature_weights, x='weight', y='feature', color='#4878CF')\n",
    "plt.title('Feature Weights')\n",
    "plt.ylabel('')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('feature_weights.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of predictions per color\n",
    "prediction_certainties_final['color'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, dof, expected = chi2_contingency(pd.crosstab(X_test_reliability['color'], X_test_reliability['is_residual']))\n",
    "\n",
    "print(f\"Chi-square test statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "\n",
    "alpha = 0.05\n",
    "if p < alpha:\n",
    "    print(\"There is a statistically significant difference between the proportions in the different groups.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference between the proportions in the different groups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_non_abs = (y_reliability_obs - y_pred)\n",
    "\n",
    "X_train_reliability, X_test_reliability, y_train_reliability, residuals_non_absolute = train_test_split(reliability_final.drop(columns=['residual']), residuals_non_abs, test_size=0.40805, random_state=808)\n",
    "\n",
    "prediction_certainties_final['non_abs_residual'] = residuals_non_absolute\n",
    "# rename non_abs_residual to type I error, type II error and correct\n",
    "prediction_certainties_final['non_abs_residual'] = prediction_certainties_final['non_abs_residual'].apply(lambda x: 'Type I error' if x == -1 else ('Type II error' if x == 1 else 'Correct'))\n",
    "\n",
    "cross_tab = pd.crosstab(prediction_certainties_final['color'], prediction_certainties_final['non_abs_residual'], normalize='index')\n",
    "\n",
    "# run a chi2 test\n",
    "chi2, p, dof, expected = chi2_contingency(cross_tab)\n",
    "\n",
    "print(f\"Chi-square test statistic: {chi2}\")\n",
    "print(f\"P-value: {p}\")\n",
    "\n",
    "# visualize the cross tab\n",
    "cross_tab = cross_tab.reindex(labels[::-1])\n",
    "ax = cross_tab.plot(kind='bar', stacked=False)\n",
    "\n",
    "# Move legend to the bottom right corner\n",
    "ax.legend(loc='upper right', title='non_abs_residual', bbox_to_anchor=(1.1, 1))\n",
    "\n",
    "plt.title('Proportion of Prediction Reliability by Residual')\n",
    "plt.xlabel('Prediction Reliability')\n",
    "plt.ylabel('Proportion')\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
